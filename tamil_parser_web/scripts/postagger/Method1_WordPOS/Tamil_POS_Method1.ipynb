{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "model_name = \"monsoon-nlp/tamillion\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, pretrained_embeddings, dropout = 0.3):\n",
    "        ''' Initialize the layers of this model.'''\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding layer that turns words into a vector of a specified size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "        # the LSTM takes embedded word vectors (of a specified size) as inputs\n",
    "        # and outputs hidden states of size hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "        # initialize the hidden state (see code below)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        ''' At the start of training, we need to initialize a hidden state;\n",
    "           there will be none because the hidden state is formed based on perviously seen data.\n",
    "           So, this function defines a hidden state with all zeroes and of a specified size.'''\n",
    "        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
    "        return (torch.zeros(2, 1, self.hidden_dim),\n",
    "                torch.zeros(2, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        ''' Define the feedforward behavior of the model.'''\n",
    "        # create embedded word vectors for each word in a sentence\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "\n",
    "        # get the output and hidden state by passing the lstm over our word embeddings\n",
    "        # the lstm takes in our embeddings and hiddent state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # get the scores for the most likely tag for a word\n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {'DET': 0, 'NUM': 1, 'NOUN': 2, 'PART': 3, 'SCONJ': 4, 'ADJ': 5, 'VERB': 6, 'PUNCT': 7, 'PROPN': 8, 'AUX': 9, 'ADP': 10, 'ADV': 11, 'PRON': 12, 'SYM': 13, 'CCONJ': 14, 'CONJ': 15, 'INTJ': 16}\n",
    "idx2tag = {0: 'DET', 1: 'NUM', 2: 'NOUN', 3: 'PART', 4: 'SCONJ', 5: 'ADJ', 6: 'VERB', 7: 'PUNCT', 8: 'PROPN', 9: 'AUX', 10: 'ADP', 11: 'ADV', 12: 'PRON', 13: 'SYM', 14: 'CCONJ', 15: 'CONJ', 16: 'INTJ'}\n",
    "\n",
    "model_vocab = tokenizer.get_vocab()\n",
    "EMBEDDING_SIZE = 300\n",
    "embedding_weights = np.random.uniform(-0.05, 0.05, size=(len(model_vocab), EMBEDDING_SIZE))\n",
    "embedding_weights = torch.from_numpy(embedding_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "# instantiate our model\n",
    "model = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(model_vocab), len(tag2idx), embedding_weights)\n",
    "\n",
    "# define our loss and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"Tamil_POS_BertTokenizer_Method1_savedModel.pth\"\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2379, 2417, 0, 8939, 2148, 2930, 0, 2127, 18]\n",
      "\n",
      "\n",
      "Predicted tags: \n",
      " tensor([12, 10,  2,  6, 11,  5,  2,  6,  7])\n",
      "\n",
      "இதன்\tPRON\n",
      "மூலம்\tADP\n",
      "ஆண்மையை\tNOUN\n",
      "அதிகரிக்கும்\tVERB\n",
      "மிக\tADV\n",
      "சிறந்த\tADJ\n",
      "ஜூஸாக\tNOUN\n",
      "உள்ளது\tVERB\n",
      ".\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"இதன் மூலம் ஆண்மையை அதிகரிக்கும் மிக சிறந்த ஜூஸாக உள்ளது .\".split()\n",
    "\n",
    "inputs = torch.tensor(tokenizer.encode(test_sentence, add_special_tokens=False))\n",
    "inputs = inputs\n",
    "tag_scores = model(inputs)\n",
    "# print(tag_scores)\n",
    "\n",
    "_, predicted_tags = torch.max(tag_scores, 1)\n",
    "print(tokenizer.encode(test_sentence, add_special_tokens=False))\n",
    "print('\\n')\n",
    "print('Predicted tags: \\n',predicted_tags)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# print(idx2tag)\n",
    "ct = 0\n",
    "for i in predicted_tags:\n",
    "    print(test_sentence[ct], end=\"\\t\")\n",
    "    print(idx2tag[i.item()])\n",
    "    ct += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
